mypipe {

  # Avro schema repository client class name
  # schema-repo-client = "mypipe.avro.schema.SchemaRepo"

  include-event-condition = """table != "binlogpos" """

  flush-interval-seconds = 10

  group-events-by-tx = true

  error {
    quit-on-empty-mutation-commit-failure = false
  }

    # consumers represent sources for mysql binary logs
  consumers { // 可以多个不同的地址
    localhost {
      # database "host:port:user:pass" array
      source = "localhost:3306:root:root"
    }

    remote {
      source = "localhost:3306:root:root"
    }

  }

  # data producers export data out (stdout, other stores, external services, etc.)
  producers {

    stdout {
      class = "mypipe.kafka.producer.stdout.StdoutProducer"
    }

    kafka-generic {
      class = "mypipe.kafka.producer.json.KafkaMutationJsonProducer"
    }
  }

  # pipes join consumers and producers 把 consumer 和 producer 组装到一起, 每个 pipe 都会生成一个 consumer
  pipes {

    stdout-pipe {
      enabled = false // 是否生效
      consumers = ["localhost"] // 只会选择第一个
      producer {
        stdout {}
      }
      # how to save and load binary log positions
      binlog-position-repo {
        # saved to a file, this is the default if unspecified
        class = "mypipe.api.repo.ConfigurableFileBasedBinaryLogPositionRepository"
        config {
          file-prefix = "log" # required if binlog-position-repo is specifiec
          data-dir = "/tmp/mypipe/data" # defaults to mypipe.data-dir if not present
        }
      }
    }

    kafka-pipe {
      enabled = true
      consumers = ["localhost"]
      producer {
        kafka-generic {
          metadata-brokers = "localhost:9092"
          include-tables = ["t_test"]
        }
      }
      binlog-position-repo {
        # saves to a MySQL database, make sure you use the following as well to prevent reacting on
        # inserts / updates made in the same DB being listenened on for changes
        class = "mypipe.api.repo.ConfigurableMySQLBasedBinaryLogPositionRepository"
        config {
          # database "host:port:user:pass" array
          source = "localhost:3306:root:root"
          database = "mypipe"
          table = "binlogpos"
          id = "kafka-generic" # used to find the row in the table for this pipe
        }
      }
    }
  }
}